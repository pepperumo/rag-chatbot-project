# LLM Routing Multi-Agent System Configuration
# Copy this file to .env and fill in your actual values

# ===== LLM Configuration =====
# Provider: openai, anthropic, gemini, ollama, etc.
LLM_PROVIDER=openai
# Your LLM API key
LLM_API_KEY=sk-your-openai-api-key-here
# Main model for search agents (e.g., gpt-4.1-mini, gpt-4.1, claude-4-sonnet)
LLM_CHOICE=gpt-4.1-mini
# Smaller model for fast routing decisions (e.g., gpt-4.1-nano)
LLM_CHOICE_SMALL=gpt-4.1-nano
# Base URL for the LLM API (change for Ollama or other providers)
LLM_BASE_URL=https://api.openai.com/v1

# Supabase configuration
# Get these from your Supabase project settings -> API
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=
SUPABASE_SERVICE_KEY=

# ===== Brave Search Configuration =====
# Get your API key from: https://api.search.brave.com/register
BRAVE_API_KEY=BSA-your-brave-search-api-key-here

# ===== Langfuse Configuration =====
# Langfuse for agent observability (optional)
# Leave these empty except the host to disable Langfuse tracing
# Get your keys from https://cloud.langfuse.com/ after creating a project
# Langfuse host will be http://localhost:3000 for the Local AI Package and https://us.cloud.langfuse.com for cloud (US example)
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_HOST=https://us.cloud.langfuse.com

# ===== Application Configuration =====
# Environment: development, staging, production
APP_ENV=development
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO
# Enable debug mode (true/false)
DEBUG=false

# ===== Routing System Configuration =====
# Cost optimization: Router uses LLM_CHOICE_SMALL for fast routing decisions
# Agent execution: Each specialized agent uses LLM_CHOICE for search tasks
# - web_search: Routes to Brave API for current events and web content  
# - email_search: Routes to Gmail readonly for email searches
# - rag_search: Routes to document search for knowledge base queries
# - fallback: Routes to general assistant for unclear requests

# ===== Example Provider Configurations =====

# For Ollama (Local):
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama2

# For Anthropic Claude:
# LLM_PROVIDER=anthropic
# LLM_BASE_URL=https://api.anthropic.com/v1
# LLM_API_KEY=sk-ant-your-key-here
# LLM_MODEL=claude-3-opus-20240229

# For Google Gemini:
# LLM_PROVIDER=gemini
# LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta
# LLM_API_KEY=your-gemini-api-key
# LLM_MODEL=gemini-pro